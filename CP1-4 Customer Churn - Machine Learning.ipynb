{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Capstone Project #1</h1>\n",
    "<h2>Predicting Customer Churn</h2>\n",
    "<h3>Cliff Robbins</h3>\n",
    "\n",
    "<h3>Proposal</h3>\n",
    "<p>My project will focus on a problem that 28 million business face each day of operation, customer churn.</p>\n",
    "\n",
    "<h3>Description:</h3>\n",
    "<p><strong>Customer churn</strong>, also known as customer attrition, customer turnover or customer defection is the loss of clients or customers.  Many companies include customer churn rate as part of their monitoring metrics because the cost of retaining current customers compared to acquiring new customers is much less.  \n",
    "Within customer churn there is the concept of voluntary and involuntary churn with voluntary being a customer leaves on their own choice while involuntary could be attributed to customer relocation to a long term care facility, death or customer relocation in a different state/geography.  In most analytical models, involuntary churn is excluded from the metric.\n",
    "</p>\n",
    "\n",
    "<h3>Formulation of a Question</h3>\n",
    "<p>When a company first starts up, the founding members can typically handle all of the various customer concerns.  As the company continues to grow, the founders can no longer service all of the various clients with support handled by a customer service team.  The customer service team focuses on current issues and a proactive approach is lost.</p>\n",
    "<p>As the company grows, the company still cares about its clients; however, due to the large customer base they can no longer address each and every customer.  This is a real problem for companies.  How does a company proactively predict if a customer is happy or unhappy?  How does a company know if a customer is so unhappy that they are willing to leave?  If a company knew if a customer was getting ready to leave, could they reach out to the customer and mend the relationship?</p>\n",
    "<h3>Hypothesis</h3>\n",
    "<p>I believe past customer data can predict future customer churn. </p>\n",
    "<h3>Prediction</h3>\n",
    "<p>If I had past customer data that showed various features and whether they stayed or churned we could use that data to predict future outcomes of current customers.</p>\n",
    "<h3>Testing</h3>\n",
    "<p>To test my hypothesis, I will use a set of customer data with various features along with whether they churned or not.</p>\n",
    "<p>The data has 7043 rows and can be found at:</p>\n",
    "<p>https://www.kaggle.com/blastchar/telco-customer-churn</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Beyond Baseline Machine Learning</h2>\n",
    "In the previous notebook, I fit the data to a Logistic Regression model using L1 and L2 regularization.  The accuracy was 75% for L1 and 74% for L2.  The performance of the model showed in imbalance regarding customers that churned.  The F1 score for customers that did not churn was 74% and the F1 score for customers that did churn was 27% (for L1 regularization).<br><br>\n",
    "The main score I am optimizing is the performance recall for the 'True' class.  Because the client is more concerned about catching customers before they churn it is okay to have false positives.  When using the base Logistic Regression recall is 93% for 'False' and 23% for 'True'.<br><br>\n",
    "This is indicative of the data set where the customers that churned have a much lower percentage compared to those that did not churn.  This is balance classification issue which cannot be fixed with throwing more data at it because there is a natural imbalance between the classes.<br><br>\n",
    "In this notebook I will use different models and data sampling techniques to test if the accuracy and/or performance improves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7043, 7)\n",
      "(7043,)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "original_recall_scores = {}\n",
    "resampled_recall_scores = {}\n",
    "\n",
    "# Load features and labels\n",
    "X = np.load('data/cp1-3-X-data.npy')\n",
    "y = np.load('data/cp1-3-y-data.npy')\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data and apply oversampling to the label (y) using SMOTE to the training data only.<br>\n",
    "I am using oversampling because there are only 7043 entries.  There is not alot of data and it is not recommended to undersample if your dataset is considered small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original test shape Counter({False: 3880, True: 1402})\n",
      "Resample test shape Counter({False: 3880, True: 3880})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE \n",
    "\n",
    "# REF https://imbalanced-learn.org/en/stable/generated/imblearn.over_sampling.SMOTE.html\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,random_state=42,stratify=y)\n",
    "\n",
    "sm = SMOTE(random_state=42)\n",
    "X_train_res,y_train_res = sm.fit_resample(X_train,y_train)\n",
    "\n",
    "print('Original test shape %s' % Counter(y_train))\n",
    "print('Resample test shape %s' % Counter(y_train_res))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Model Selection</h2>\n",
    "Now that the data is prepared, I will us a variety of models and then compare the accuracy and performance of the models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Logistic Regression</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data Set\n",
      "Accuracy Score 0.7444633730834753\n",
      "[[1205   89]\n",
      " [ 361  106]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.77      0.93      0.84      1294\n",
      "        True       0.54      0.23      0.32       467\n",
      "\n",
      "   micro avg       0.74      0.74      0.74      1761\n",
      "   macro avg       0.66      0.58      0.58      1761\n",
      "weighted avg       0.71      0.74      0.70      1761\n",
      "\n",
      "\n",
      "Resampled Data Set\n",
      "Accuracy Score 0.6757524134014764\n",
      "[[814 480]\n",
      " [ 91 376]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.90      0.63      0.74      1294\n",
      "        True       0.44      0.81      0.57       467\n",
      "\n",
      "   micro avg       0.68      0.68      0.68      1761\n",
      "   macro avg       0.67      0.72      0.65      1761\n",
      "weighted avg       0.78      0.68      0.69      1761\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cliff/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/cliff/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, recall_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# REF https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
    "\n",
    "# Original Data set\n",
    "lr = LogisticRegression(penalty='l1',C=0.1)\n",
    "lr.fit(X_train,y_train)\n",
    "y_pred = lr.predict(X_test)\n",
    "accuracy = accuracy_score(y_pred,y_test)\n",
    "original_recall_scores['Logistic Regression'] = np.append(recall_score(y_test, y_pred,labels=[0,1],average=None),accuracy)\n",
    "\n",
    "# Resample Data set\n",
    "lr_res = LogisticRegression(penalty='l1',C=0.1)\n",
    "lr_res.fit(X_train_res,y_train_res)\n",
    "y_pred_res = lr_res.predict(X_test)\n",
    "accuracy_res = accuracy_score(y_pred_res,y_test)\n",
    "resampled_recall_scores['SMOTE - Logistic Regression'] = np.append(recall_score(y_test, y_pred_res,labels=[0,1],average=None),accuracy_res)\n",
    "\n",
    "print('Original Data Set')\n",
    "print('Accuracy Score %s' % accuracy)\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "print()\n",
    "print('Resampled Data Set')\n",
    "print('Accuracy Score %s' % accuracy_res)\n",
    "print(confusion_matrix(y_test, y_pred_res))\n",
    "print(classification_report(y_test, y_pred_res))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Naïve Bayes</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data Set\n",
      "Accuracy Score 0.7410562180579217\n",
      "[[1005  289]\n",
      " [ 167  300]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.86      0.78      0.82      1294\n",
      "        True       0.51      0.64      0.57       467\n",
      "\n",
      "   micro avg       0.74      0.74      0.74      1761\n",
      "   macro avg       0.68      0.71      0.69      1761\n",
      "weighted avg       0.77      0.74      0.75      1761\n",
      "\n",
      "\n",
      "Resampled Data Set\n",
      "Accuracy Score 0.6990346394094265\n",
      "[[870 424]\n",
      " [106 361]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.89      0.67      0.77      1294\n",
      "        True       0.46      0.77      0.58       467\n",
      "\n",
      "   micro avg       0.70      0.70      0.70      1761\n",
      "   macro avg       0.68      0.72      0.67      1761\n",
      "weighted avg       0.78      0.70      0.72      1761\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, recall_score\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "# REF https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html\n",
    "\n",
    "# Original Data set\n",
    "nb = GaussianNB()\n",
    "nb.fit(X_train,y_train)\n",
    "y_pred = nb.predict(X_test)\n",
    "accuracy = accuracy_score(y_pred,y_test)\n",
    "original_recall_scores['Naïve Bayes'] = np.append(recall_score(y_test, y_pred,labels=[0,1],average=None),accuracy)\n",
    "\n",
    "# Resample Data set\n",
    "nb_res = GaussianNB()\n",
    "nb_res.fit(X_train_res,y_train_res)\n",
    "y_pred_res = nb_res.predict(X_test)\n",
    "accuracy_res = accuracy_score(y_pred_res,y_test)\n",
    "resampled_recall_scores['SMOTE - Naïve Bayes'] = np.append(recall_score(y_test, y_pred_res,labels=[0,1],average=None),accuracy_res)\n",
    "\n",
    "print('Original Data Set')\n",
    "print('Accuracy Score %s' % accuracy)\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "print()\n",
    "print('Resampled Data Set')\n",
    "print('Accuracy Score %s' % accuracy_res)\n",
    "print(confusion_matrix(y_test, y_pred_res))\n",
    "print(classification_report(y_test, y_pred_res))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Decision Tree</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data Set\n",
      "Accuracy Score 0.7563884156729132\n",
      "[[1129  165]\n",
      " [ 264  203]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.81      0.87      0.84      1294\n",
      "        True       0.55      0.43      0.49       467\n",
      "\n",
      "   micro avg       0.76      0.76      0.76      1761\n",
      "   macro avg       0.68      0.65      0.66      1761\n",
      "weighted avg       0.74      0.76      0.75      1761\n",
      "\n",
      "\n",
      "Resampled Data Set\n",
      "Accuracy Score 0.7132311186825667\n",
      "[[903 391]\n",
      " [114 353]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.89      0.70      0.78      1294\n",
      "        True       0.47      0.76      0.58       467\n",
      "\n",
      "   micro avg       0.71      0.71      0.71      1761\n",
      "   macro avg       0.68      0.73      0.68      1761\n",
      "weighted avg       0.78      0.71      0.73      1761\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, recall_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "# REF https://scikit-learn.org/stable/modules/tree.html\n",
    "\n",
    "depth = 6 # Tested various depths from 3 to 10\n",
    "\n",
    "# Original Data set\n",
    "tree = DecisionTreeClassifier(max_depth=depth,random_state=42,max_features=None,min_samples_leaf=15)\n",
    "tree.fit(X_train,y_train)\n",
    "y_pred = tree.predict(X_test)\n",
    "accuracy = accuracy_score(y_pred,y_test)\n",
    "original_recall_scores['Decision Tree'] = np.append(recall_score(y_test, y_pred,labels=[0,1],average=None),accuracy)\n",
    "\n",
    "# Resample Data set\n",
    "tree_res = DecisionTreeClassifier(max_depth=depth,random_state=42,max_features=None,min_samples_leaf=15)\n",
    "tree_res.fit(X_train_res,y_train_res)\n",
    "y_pred_res = tree_res.predict(X_test)\n",
    "accuracy_res = accuracy_score(y_pred_res,y_test)\n",
    "resampled_recall_scores['SMOTE - Decision Tree'] = np.append(recall_score(y_test, y_pred_res,labels=[0,1],average=None),accuracy_res)\n",
    "\n",
    "print('Original Data Set')\n",
    "print('Accuracy Score %s' % accuracy)\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "print()\n",
    "print('Resampled Data Set')\n",
    "print('Accuracy Score %s' % accuracy_res)\n",
    "print(confusion_matrix(y_test, y_pred_res))\n",
    "print(classification_report(y_test, y_pred_res))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>kNN</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data Set\n",
      "Accuracy Score 0.7438955139125497\n",
      "[[1107  187]\n",
      " [ 264  203]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.81      0.86      0.83      1294\n",
      "        True       0.52      0.43      0.47       467\n",
      "\n",
      "   micro avg       0.74      0.74      0.74      1761\n",
      "   macro avg       0.66      0.65      0.65      1761\n",
      "weighted avg       0.73      0.74      0.74      1761\n",
      "\n",
      "\n",
      "Resampled Data Set\n",
      "Accuracy Score 0.6876774559909142\n",
      "[[1038  256]\n",
      " [ 294  173]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.78      0.80      0.79      1294\n",
      "        True       0.40      0.37      0.39       467\n",
      "\n",
      "   micro avg       0.69      0.69      0.69      1761\n",
      "   macro avg       0.59      0.59      0.59      1761\n",
      "weighted avg       0.68      0.69      0.68      1761\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, recall_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "# REF https://scikit-learn.org/stable/modules/neighbors.html\n",
    "\n",
    "# The lower k = more complex model and can lead to overfitting.\n",
    "# The higher k = less complex model and can lead to underfitting.\n",
    "neighbors = 7 # Tested various depths from 1 to 10\n",
    "\n",
    "# Original Data set\n",
    "knn = KNeighborsClassifier(n_neighbors=neighbors)\n",
    "knn.fit(X_train,y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "accuracy = accuracy_score(y_pred,y_test)\n",
    "original_recall_scores['kNN'] = np.append(recall_score(y_test, y_pred,labels=[0,1],average=None),accuracy)\n",
    "\n",
    "# Resample Data set\n",
    "knn_res = KNeighborsClassifier(n_neighbors=neighbors)\n",
    "knn_res.fit(X_train_res,y_train_res)\n",
    "y_pred_res = knn_res.predict(X_test)\n",
    "accuracy_res = accuracy_score(y_pred_res,y_test)\n",
    "resampled_recall_scores['SMOTE - kNN'] = np.append(recall_score(y_test, y_pred_res,labels=[0,1],average=None),accuracy_res)\n",
    "\n",
    "print('Original Data Set')\n",
    "print('Accuracy Score %s' % accuracy)\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "print()\n",
    "print('Resampled Data Set')\n",
    "print('Accuracy Score %s' % accuracy_res)\n",
    "print(confusion_matrix(y_test, y_pred_res))\n",
    "print(classification_report(y_test, y_pred_res))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>SVM</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data Set\n",
      "Accuracy Score 0.7563884156729132\n",
      "[[1119  175]\n",
      " [ 254  213]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.82      0.86      0.84      1294\n",
      "        True       0.55      0.46      0.50       467\n",
      "\n",
      "   micro avg       0.76      0.76      0.76      1761\n",
      "   macro avg       0.68      0.66      0.67      1761\n",
      "weighted avg       0.74      0.76      0.75      1761\n",
      "\n",
      "\n",
      "Resampled Data Set\n",
      "Accuracy Score 0.6973310618966496\n",
      "[[841 453]\n",
      " [ 80 387]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.91      0.65      0.76      1294\n",
      "        True       0.46      0.83      0.59       467\n",
      "\n",
      "   micro avg       0.70      0.70      0.70      1761\n",
      "   macro avg       0.69      0.74      0.68      1761\n",
      "weighted avg       0.79      0.70      0.72      1761\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, recall_score\n",
    "from sklearn.svm import SVC\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html\n",
    "\n",
    "c = 0.8 # Tested various depths from 1 - .025\n",
    "\n",
    "# Original Data set\n",
    "svm = SVC(C=c,random_state=42,gamma='auto')\n",
    "svm.fit(X_train,y_train)\n",
    "y_pred = svm.predict(X_test)\n",
    "accuracy = accuracy_score(y_pred,y_test)\n",
    "original_recall_scores['SVM'] = np.append(recall_score(y_test, y_pred,labels=[0,1],average=None),accuracy)\n",
    "\n",
    "# Resample Data set\n",
    "svm_res = SVC(C=c,random_state=42,gamma='auto')\n",
    "svm_res.fit(X_train_res,y_train_res)\n",
    "y_pred_res = svm_res.predict(X_test)\n",
    "accuracy_res = accuracy_score(y_pred_res,y_test)\n",
    "resampled_recall_scores['SMOTE - SVM'] = np.append(recall_score(y_test, y_pred_res,labels=[0,1],average=None),accuracy_res)\n",
    "\n",
    "print('Original Data Set')\n",
    "print('Accuracy Score %s' % accuracy)\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "print()\n",
    "print('Resampled Data Set')\n",
    "print('Accuracy Score %s' % accuracy_res)\n",
    "print(confusion_matrix(y_test, y_pred_res))\n",
    "print(classification_report(y_test, y_pred_res))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Random Forest with Random Hyperparameter Grid Search</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:   22.5s\n",
      "/Users/cliff/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:  2.0min\n",
      "[Parallel(n_jobs=-1)]: Done 300 out of 300 | elapsed:  3.7min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:   15.3s\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 300 out of 300 | elapsed:  2.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data Set\n",
      "Accuracy Score 0.7586598523566156\n",
      "[[1125  169]\n",
      " [ 256  211]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.81      0.87      0.84      1294\n",
      "        True       0.56      0.45      0.50       467\n",
      "\n",
      "   micro avg       0.76      0.76      0.76      1761\n",
      "   macro avg       0.68      0.66      0.67      1761\n",
      "weighted avg       0.75      0.76      0.75      1761\n",
      "\n",
      "{'n_estimators': 200, 'min_samples_split': 5, 'min_samples_leaf': 4, 'max_features': 'auto', 'max_depth': 80, 'bootstrap': True}\n",
      "\n",
      "Resampled Data Set\n",
      "Accuracy Score 0.7126632595116411\n",
      "[[898 396]\n",
      " [110 357]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.89      0.69      0.78      1294\n",
      "        True       0.47      0.76      0.59       467\n",
      "\n",
      "   micro avg       0.71      0.71      0.71      1761\n",
      "   macro avg       0.68      0.73      0.68      1761\n",
      "weighted avg       0.78      0.71      0.73      1761\n",
      "\n",
      "{'n_estimators': 200, 'min_samples_split': 5, 'min_samples_leaf': 4, 'max_features': 'auto', 'max_depth': 80, 'bootstrap': True}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n",
    "\n",
    "# Hyperparemeter testing\n",
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "\n",
    "# Original Data set\n",
    "rfm = RandomForestClassifier()\n",
    "rfm_random = RandomizedSearchCV(estimator = rfm, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n",
    "rfm_random.fit(X_train,y_train)\n",
    "y_pred = rfm_random.predict(X_test)\n",
    "accuracy = accuracy_score(y_pred,y_test)\n",
    "\n",
    "# Resample Data set\n",
    "rfm_res = RandomForestClassifier()\n",
    "rfm_random_res = RandomizedSearchCV(estimator = rfm, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n",
    "rfm_random_res.fit(X_train_res,y_train_res)\n",
    "y_pred_res = rfm_random_res.predict(X_test)\n",
    "accuracy_res = accuracy_score(y_pred_res,y_test)\n",
    "\n",
    "print('Original Data Set')\n",
    "print('Accuracy Score %s' % accuracy)\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(rfm_random.best_params_)\n",
    "print()\n",
    "print('Resampled Data Set')\n",
    "print('Accuracy Score %s' % accuracy_res)\n",
    "print(confusion_matrix(y_test, y_pred_res))\n",
    "print(classification_report(y_test, y_pred_res))\n",
    "print(rfm_random_res.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Random Forest with Hyperparameter Grid Search</h2>\n",
    "We will take the best params from above and rerun the search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 360 candidates, totalling 1080 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    4.4s\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:   30.4s\n",
      "[Parallel(n_jobs=-1)]: Done 349 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 632 tasks      | elapsed:  2.3min\n",
      "[Parallel(n_jobs=-1)]: Done 997 tasks      | elapsed:  3.8min\n",
      "[Parallel(n_jobs=-1)]: Done 1080 out of 1080 | elapsed:  4.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 504 candidates, totalling 1512 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:   10.9s\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 349 tasks      | elapsed:  2.6min\n",
      "[Parallel(n_jobs=-1)]: Done 632 tasks      | elapsed:  4.9min\n",
      "[Parallel(n_jobs=-1)]: Done 997 tasks      | elapsed:  7.5min\n",
      "[Parallel(n_jobs=-1)]: Done 1442 tasks      | elapsed: 10.7min\n",
      "[Parallel(n_jobs=-1)]: Done 1512 out of 1512 | elapsed: 11.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data Set\n",
      "Accuracy Score 0.7620670073821693\n",
      "[[1123  171]\n",
      " [ 248  219]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.82      0.87      0.84      1294\n",
      "        True       0.56      0.47      0.51       467\n",
      "\n",
      "   micro avg       0.76      0.76      0.76      1761\n",
      "   macro avg       0.69      0.67      0.68      1761\n",
      "weighted avg       0.75      0.76      0.75      1761\n",
      "\n",
      "{'bootstrap': True, 'max_depth': 90, 'max_features': 'auto', 'min_samples_leaf': 4, 'min_samples_split': 10, 'n_estimators': 100}\n",
      "\n",
      "Resampled Data Set\n",
      "Accuracy Score 0.7223168654173765\n",
      "[[899 395]\n",
      " [ 94 373]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.91      0.69      0.79      1294\n",
      "        True       0.49      0.80      0.60       467\n",
      "\n",
      "   micro avg       0.72      0.72      0.72      1761\n",
      "   macro avg       0.70      0.75      0.70      1761\n",
      "weighted avg       0.79      0.72      0.74      1761\n",
      "\n",
      "{'bootstrap': True, 'max_depth': 80, 'max_features': 'auto', 'min_samples_leaf': 5, 'min_samples_split': 2, 'n_estimators': 500}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n",
    "\n",
    "# Hyperparemeter testing\n",
    "\n",
    "# Create the parameter grid based on the results of random search \n",
    "param_grid = {\n",
    "    'bootstrap': [True],\n",
    "    'max_depth': [80, 90, 100, 110],\n",
    "    'max_features': ['auto', 'sqrt'],\n",
    "    'min_samples_leaf': [3, 4, 5],\n",
    "    'min_samples_split': [8, 10, 12],\n",
    "    'n_estimators': [100, 200, 300,400, 1000]\n",
    "}\n",
    "\n",
    "param_grid_res = {\n",
    "    'bootstrap': [True],\n",
    "    'max_depth': [40, 50, 70, 80],\n",
    "    'max_features': ['auto', 'sqrt'],\n",
    "    'min_samples_leaf': [3, 4, 5],\n",
    "    'min_samples_split': [ 2, 4,8],\n",
    "    'n_estimators': [300,400,500,600,700,800,1000]\n",
    "}\n",
    "\n",
    "\n",
    "# Original Data set\n",
    "rfm = RandomForestClassifier()\n",
    "rfm_random = GridSearchCV(estimator = rfm, param_grid = param_grid, cv = 3, verbose=2, n_jobs = -1)\n",
    "rfm_random.fit(X_train,y_train)\n",
    "y_pred = rfm_random.predict(X_test)\n",
    "accuracy = accuracy_score(y_pred,y_test)\n",
    "\n",
    "# Resample Data set\n",
    "rfm_res = RandomForestClassifier()\n",
    "rfm_random_res = GridSearchCV(estimator = rfm, param_grid = param_grid_res, cv = 3, verbose=2, n_jobs = -1)\n",
    "rfm_random_res.fit(X_train_res,y_train_res)\n",
    "y_pred_res = rfm_random_res.predict(X_test)\n",
    "accuracy_res = accuracy_score(y_pred_res,y_test)\n",
    "\n",
    "print('Original Data Set')\n",
    "print('Accuracy Score %s' % accuracy)\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(rfm_random.best_params_)\n",
    "print()\n",
    "print('Resampled Data Set')\n",
    "print('Accuracy Score %s' % accuracy_res)\n",
    "print(confusion_matrix(y_test, y_pred_res))\n",
    "print(classification_report(y_test, y_pred_res))\n",
    "print(rfm_random_res.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Random Forest with Best Estimators</h2>\n",
    "I am applying the best estimators to the Random Forest model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data Set\n",
      "Accuracy Score 0.7620670073821693\n",
      "[[1122  172]\n",
      " [ 247  220]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.82      0.87      0.84      1294\n",
      "        True       0.56      0.47      0.51       467\n",
      "\n",
      "   micro avg       0.76      0.76      0.76      1761\n",
      "   macro avg       0.69      0.67      0.68      1761\n",
      "weighted avg       0.75      0.76      0.76      1761\n",
      "\n",
      "\n",
      "Resampled Data Set\n",
      "Accuracy Score 0.7115275411697899\n",
      "[[897 397]\n",
      " [111 356]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.89      0.69      0.78      1294\n",
      "        True       0.47      0.76      0.58       467\n",
      "\n",
      "   micro avg       0.71      0.71      0.71      1761\n",
      "   macro avg       0.68      0.73      0.68      1761\n",
      "weighted avg       0.78      0.71      0.73      1761\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, recall_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n",
    "\n",
    "# Hyperparemeter testing\n",
    "\n",
    "# Create the parameter grid based on the results of random search \n",
    "param_grid = {\n",
    "    'bootstrap': [True],\n",
    "    'max_depth': [80, 90, 100, 110],\n",
    "    'max_features': ['auto', 'sqrt'],\n",
    "    'min_samples_leaf': [3, 4, 5],\n",
    "    'min_samples_split': [8, 10, 12],\n",
    "    'n_estimators': [100, 200, 300,400, 1000]\n",
    "}\n",
    "\n",
    "param_grid_res = {\n",
    "    'bootstrap': [True],\n",
    "    'max_depth': [40, 50, 70, 80],\n",
    "    'max_features': ['auto', 'sqrt'],\n",
    "    'min_samples_leaf': [3, 4, 5],\n",
    "    'min_samples_split': [ 2, 4,8],\n",
    "    'n_estimators': [300,400,500,600,700,800,1000]\n",
    "}\n",
    "\n",
    "\n",
    "# Original Data set\n",
    "rfm = RandomForestClassifier(bootstrap=True, max_depth=90, max_features='auto', min_samples_leaf=4, min_samples_split=10, n_estimators=100)\n",
    "rfm.fit(X_train,y_train)\n",
    "y_pred = rfm.predict(X_test)\n",
    "accuracy = accuracy_score(y_pred,y_test)\n",
    "original_recall_scores['Random Forest'] = np.append(recall_score(y_test, y_pred,labels=[0,1],average=None),accuracy)\n",
    "\n",
    "# Resample Data set\n",
    "rfm_res = RandomForestClassifier(bootstrap=True, max_depth=80, max_features='auto', min_samples_leaf=5, min_samples_split=2, n_estimators=500)\n",
    "rfm_res.fit(X_train_res,y_train_res)\n",
    "y_pred_res = rfm_res.predict(X_test)\n",
    "accuracy_res = accuracy_score(y_pred_res,y_test)\n",
    "resampled_recall_scores['SMOTE - Random Forest'] = np.append(recall_score(y_test, y_pred_res,labels=[0,1],average=None),accuracy_res)\n",
    "\n",
    "print('Original Data Set')\n",
    "print('Accuracy Score %s' % accuracy)\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "print()\n",
    "print('Resampled Data Set')\n",
    "print('Accuracy Score %s' % accuracy_res)\n",
    "print(confusion_matrix(y_test, y_pred_res))\n",
    "print(classification_report(y_test, y_pred_res))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>AdaBoost</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data Set\n",
      "Accuracy Score 0.7643384440658717\n",
      "[[1135  159]\n",
      " [ 256  211]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.82      0.88      0.85      1294\n",
      "        True       0.57      0.45      0.50       467\n",
      "\n",
      "   micro avg       0.76      0.76      0.76      1761\n",
      "   macro avg       0.69      0.66      0.67      1761\n",
      "weighted avg       0.75      0.76      0.75      1761\n",
      "\n",
      "\n",
      "Resampled Data Set\n",
      "Accuracy Score 0.7262918796138558\n",
      "[[913 381]\n",
      " [101 366]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.90      0.71      0.79      1294\n",
      "        True       0.49      0.78      0.60       467\n",
      "\n",
      "   micro avg       0.73      0.73      0.73      1761\n",
      "   macro avg       0.70      0.74      0.70      1761\n",
      "weighted avg       0.79      0.73      0.74      1761\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, recall_score\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "# REF https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html\n",
    "\n",
    "n = 100\n",
    "\n",
    "# Original Data set\n",
    "abc = AdaBoostClassifier(n_estimators=n, random_state=42)\n",
    "abc.fit(X_train,y_train)\n",
    "y_pred = abc.predict(X_test)\n",
    "accuracy = accuracy_score(y_pred,y_test)\n",
    "original_recall_scores['AdaBoost'] = np.append(recall_score(y_test, y_pred,labels=[0,1],average=None),accuracy)\n",
    "\n",
    "# Resample Data set\n",
    "abc_res = AdaBoostClassifier(n_estimators=n, random_state=42)\n",
    "abc_res.fit(X_train_res,y_train_res)\n",
    "y_pred_res = abc_res.predict(X_test)\n",
    "accuracy_res = accuracy_score(y_pred_res,y_test)\n",
    "resampled_recall_scores['SMOTE - AdaBoost'] = np.append(recall_score(y_test, y_pred_res,labels=[0,1],average=None),accuracy_res)\n",
    "\n",
    "print('Original Data Set')\n",
    "print('Accuracy Score %s' % accuracy)\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "print()\n",
    "print('Resampled Data Set')\n",
    "print('Accuracy Score %s' % accuracy_res)\n",
    "print(confusion_matrix(y_test, y_pred_res))\n",
    "print(classification_report(y_test, y_pred_res))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Resample with RUS and AdaBoost Model</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original test shape Counter({False: 3880, True: 1402})\n",
      "Resample test shape Counter({False: 1402, True: 1402})\n",
      "Original Data Set\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'accuracy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-78e37a6e1a45>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Original Data Set'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Accuracy Score %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassification_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'accuracy' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, recall_score\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from collections import Counter\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "# REF https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html\n",
    "# https://github.com/dialnd/imbalanced-algorithms\n",
    "\n",
    "#Resample using RUS\n",
    "rus = RandomUnderSampler(random_state=42)\n",
    "X_train_res,y_train_res = rus.fit_resample(X_train,y_train)\n",
    "\n",
    "print('Original test shape %s' % Counter(y_train))\n",
    "print('Resample test shape %s' % Counter(y_train_res))\n",
    "\n",
    "n = 100\n",
    "\n",
    "# Resample Data set\n",
    "abc_res = AdaBoostClassifier(n_estimators=n, random_state=42)\n",
    "abc_res.fit(X_train_res,y_train_res)\n",
    "y_pred_res = abc_res.predict(X_test)\n",
    "accuracy_res = accuracy_score(y_pred_res,y_test)\n",
    "resampled_recall_scores['RUS - AdaBoost'] = np.append(recall_score(y_test, y_pred_res,labels=[0,1],average=None),accuracy_res)\n",
    "\n",
    "print('Original Data Set')\n",
    "print('Accuracy Score %s' % accuracy)\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "print()\n",
    "print('Resampled Data Set')\n",
    "print('Accuracy Score %s' % accuracy_res)\n",
    "print(confusion_matrix(y_test, y_pred_res))\n",
    "print(classification_report(y_test, y_pred_res))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Leveraging RUSBoost model/resampling library</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data Set\n",
      "Accuracy Score 0.6212379329926179\n",
      "[[685 609]\n",
      " [ 58 409]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.92      0.53      0.67      1294\n",
      "        True       0.40      0.88      0.55       467\n",
      "\n",
      "   micro avg       0.62      0.62      0.62      1761\n",
      "   macro avg       0.66      0.70      0.61      1761\n",
      "weighted avg       0.78      0.62      0.64      1761\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, recall_score\n",
    "import rus\n",
    "# https://github.com/dialnd/imbalanced-algorithms\n",
    "\n",
    "n = 100\n",
    "\n",
    "# Resampled Data set\n",
    "abc = rus.RUSBoost(n_estimators=n, n_samples=300)\n",
    "abc.fit(X_train,y_train)\n",
    "y_pred = abc.predict(X_test)\n",
    "accuracy = accuracy_score(y_pred,y_test)\n",
    "resampled_recall_scores['RUS - Boost'] = np.append(recall_score(y_test, y_pred,labels=[0,1],average=None),accuracy)\n",
    "\n",
    "print('Original Data Set')\n",
    "print('Accuracy Score %s' % accuracy)\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Leveraging SMOTEBoost model/resampling library</h2>\n",
    "Refer to https://www3.nd.edu/~dial/publications/hoens2013imbalanced.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data Set\n",
      "Accuracy Score 0.5843270868824532\n",
      "[[580 714]\n",
      " [ 18 449]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.97      0.45      0.61      1294\n",
      "        True       0.39      0.96      0.55       467\n",
      "\n",
      "   micro avg       0.58      0.58      0.58      1761\n",
      "   macro avg       0.68      0.70      0.58      1761\n",
      "weighted avg       0.82      0.58      0.60      1761\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, recall_score\n",
    "import smote\n",
    "# REF https://github.com/dialnd/imbalanced-algorithms\n",
    "# REF https://www3.nd.edu/~dial/publications/hoens2013imbalanced.pdf\n",
    "\n",
    "n = 100\n",
    "\n",
    "# Resampled Data set\n",
    "abc = smote.SMOTEBoost(n_estimators=n, n_samples=300)\n",
    "abc.fit(X_train,y_train)\n",
    "y_pred = abc.predict(X_test)\n",
    "accuracy = accuracy_score(y_pred,y_test)\n",
    "resampled_recall_scores['SMOTE - Boost'] = np.append(recall_score(y_test, y_pred,labels=[0,1],average=None),accuracy)\n",
    "\n",
    "print('Original Data Set')\n",
    "print('Accuracy Score %s' % accuracy)\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Assessing results</h2>\n",
    "For problems with class imbalance, metrics such as precision, recall, and f1-score give good insight to how a classifier performs with respect to the minority class. Depending on the problem, the goal is to optimize precision and/or recall of the classifier. In this case, I want a model that catches the most number of instances of the minority class, even if it increases the number of false positives. A classifier with a high recall score will give the greatest number of potential customer churns, or at least raise a flag on most of the cases. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Dataset\n",
      "+---------------------+----------------+---------------+----------+\n",
      "|       Models        | Recall - False | Recall - True | Accuracy |\n",
      "+=====================+================+===============+==========+\n",
      "| Logistic Regression | 0.931          | 0.227         | 0.744    |\n",
      "+---------------------+----------------+---------------+----------+\n",
      "| Naïve Bayes         | 0.777          | 0.642         | 0.741    |\n",
      "+---------------------+----------------+---------------+----------+\n",
      "| Decision Tree       | 0.872          | 0.435         | 0.756    |\n",
      "+---------------------+----------------+---------------+----------+\n",
      "| kNN                 | 0.855          | 0.435         | 0.744    |\n",
      "+---------------------+----------------+---------------+----------+\n",
      "| SVM                 | 0.865          | 0.456         | 0.756    |\n",
      "+---------------------+----------------+---------------+----------+\n",
      "| Random Forest       | 0.867          | 0.471         | 0.762    |\n",
      "+---------------------+----------------+---------------+----------+\n",
      "| AdaBoost            | 0.877          | 0.452         | 0.764    |\n",
      "+---------------------+----------------+---------------+----------+\n",
      "\n",
      "Sampled Dataset\n",
      "+-----------------------------+----------------+---------------+----------+\n",
      "|           Models            | Recall - False | Recall - True | Accuracy |\n",
      "+=============================+================+===============+==========+\n",
      "| SMOTE - Logistic Regression | 0.629          | 0.805         | 0.676    |\n",
      "+-----------------------------+----------------+---------------+----------+\n",
      "| SMOTE - Naïve Bayes         | 0.672          | 0.773         | 0.699    |\n",
      "+-----------------------------+----------------+---------------+----------+\n",
      "| SMOTE - Decision Tree       | 0.698          | 0.756         | 0.713    |\n",
      "+-----------------------------+----------------+---------------+----------+\n",
      "| SMOTE - kNN                 | 0.802          | 0.370         | 0.688    |\n",
      "+-----------------------------+----------------+---------------+----------+\n",
      "| SMOTE - SVM                 | 0.650          | 0.829         | 0.697    |\n",
      "+-----------------------------+----------------+---------------+----------+\n",
      "| SMOTE - Random Forest       | 0.693          | 0.762         | 0.712    |\n",
      "+-----------------------------+----------------+---------------+----------+\n",
      "| SMOTE - AdaBoost            | 0.706          | 0.784         | 0.726    |\n",
      "+-----------------------------+----------------+---------------+----------+\n",
      "| RUS - AdaBoost              | 0.706          | 0.784         | 0.726    |\n",
      "+-----------------------------+----------------+---------------+----------+\n",
      "| RUS - Boost                 | 0.529          | 0.876         | 0.621    |\n",
      "+-----------------------------+----------------+---------------+----------+\n",
      "| SMOTE - Boost               | 0.448          | 0.961         | 0.584    |\n",
      "+-----------------------------+----------------+---------------+----------+\n"
     ]
    }
   ],
   "source": [
    "import texttable as tt\n",
    "\n",
    "# REF https://github.com/foutaise/texttable/\n",
    "\n",
    "original_data_table = tt.Texttable()\n",
    "sampled_data_table = tt.Texttable()\n",
    "headings = ['Models','Recall - False','Recall - True','Accuracy']\n",
    "original_data_table.header(headings)\n",
    "sampled_data_table.header(headings)\n",
    "\n",
    "for key,value in original_recall_scores.items():\n",
    "    original_data_table.add_row([key,value[0],value[1],value[2]])\n",
    "    \n",
    "for key,value in resampled_recall_scores.items():\n",
    "    sampled_data_table.add_row([key,value[0],value[1],value[2]])\n",
    "\n",
    "original_table = original_data_table.draw()\n",
    "sampled_table = sampled_data_table.draw()\n",
    "print('Original Dataset')\n",
    "print (original_table + '\\n')\n",
    "\n",
    "print('Sampled Dataset')\n",
    "print(sampled_table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
